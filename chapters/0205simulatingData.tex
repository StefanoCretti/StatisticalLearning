\chapter{Simulating data from a GLM in R}
Simulating data is often used to test the performance of your model.

\section{Example: simulating Poisson data}
In general, in a Possion regression you have:
$$Y_i = (Y|\vec{X} = \vec{x}_i) \sim \text{Poisson}(\mu_i)$$
$$\mu_i = E[Y_i] \in (0, +\infty)$$
$$\eta_i = \vec{x}_i^t\vec{\beta} \in (-\infty, \infty)$$

If you then decide to use the canonical link, which is the $\log$ function:
$$\eta_i = \log(\mu_i) \implies \mu_i = e^{\eta_i}$$

Therefore you can generalize this Poisson regression as
$$Y_i \sim \text{Poisson}(e^{\vec{x}_i^t\vec{\beta}})$$

Since $\eta_i = e^{\vec{x}_i^t\vec{\beta}}$. To then simulate data to test this
model you must:
\begin{enumerate}
  \item Assume a number of predictors $p$ (in our case $p=1$ for plotting
  convenience)
  \item Fix some values for $\vec{\beta}$ (in our case notice that $\eta_i =
  \beta_0 + \beta_ix_i$)
  \item Generate $n$ random values of $x_i$ (in this case $n=100$) 
  \item Compute the corresponding values of $eta_i$ (in this case, we have 100
  values of $\eta$)
  \item Compute the values of $\mu$ by using the correct link function, and
  consequently the values of $y_i$
  \item Try several methosds to find a model correctly fitting the simulated
  data
\end{enumerate} \vspace{\baselineskip}

In \underline{\textbf{R}} this translates to:
\begin{minted}{R}
  # Set the seeds 
  set.seed(123)  
  
  x <- rnorm(100, 0, 2)  
  # Does not matter the distribution, in this case we are getting random values following a
  # normal distribution with 0 mean and 2 standard deviation 
  
  beta0 <- 1  # Fixed, intercept
  beta1 <- 2  # Fixed 
  
  eta <- beta0 + beta1 * x  # generated the principal component. #TODO non ricordo il nome preciso
  # Eta assumes all the mean values of the linear model
  
  plot(x, eta) # where eta represents the mean values calculated through the components of the model
  # of course it is the points of eta stay on a line
  
  # since we consider the log function as the link function (eta = log(mu))
  # consequently mu = exp(eta)
  mu <- exp(eta) #I find the real mean values for each Yi,
  # that have to be still calculated
  
  y <- rpois(100, lambda=mu)  # generates y values accordingly to poisson distributions 
  # formulated using the values of mu calculated before. 
  
  # I put myself in a situation where I know data (x, y) but I don't know where
  # they come from. I use statistical techniques to fit my model on the data
  # generated hoping to get the model used to obtain the data
  
  plot(x, y, type = "p")  # See that is not linear, could be log(y)? 
  plot(x, log(y))  # Since we obtain a linear trend, 
  # we expect eta to be in the form beta0 + beta1x
  
  # Try fitting a line (not the best approach) 
  z <- log(y)  
  summary(z) # typical Poisson data contain several 0s, which
  # give -infinites 
  
  #Transform the data
  z <- log(y+1)  # Sudo-count transformation (commonly used) 
  summary(z)
  plot(x, log(y+1))
  
  modA <- lm(formula=z~x)  # Fit a line in z, since we found a linear behaviour
  summary(modA)  # Estimated parameters are far from the chosen ones. The estimates of the
  # betas are completely different
  
  plot(modA)  # Notice systematic trends in Residual vs 
  # Fitted analysis: it is suggested the presence of a 
  # quadratic term in the residuals. A good pattern would be a random 
  # distribution around 0. QQ plot is ok. Scale-location checks for homoskedasticity, it is not ok 
  # From the Residual vs Leverage plot it seems that there is a systematic trend of the 
  # residuals, while instead they should be distributed above and below 
  
  # add a quadratic term
  modB <- lm(z~x+I(x^2))  # Adding quadratic term, take x, produces x^2 and puts it in the formula
  summary(modB)  # Not that far from starting data
  plot(modB)  # Not the best but seems decent
  
  # Proper way to try and fit data y[1:100]  
  y[1:100]
  
  # Inspect the data and notice that the response in positive
  #discrete You could then try using generalized linear models, and in particular Poisson regression.
  # Poisson seems reasonable, which is the simplest to fit this kind of data. 
  # Negative binomial is another option, that we will not consider in this case.
  
  modC <- glm(formula=y~x, family="poisson")  # no transformation.
  # Default is gaussian, change to Poisson, it takes the canonical link
  summary(modC)d
  # Residuals are not the same in case of generalized linear models
  # Really near 1 and 2, the real values
  # Note: Because of how glm works, you need decently big values for n. In linear
  # regression models the beta hat vector has a gaussian distribution, because there
  # is an assumpion of gaussianity in the model. In the case of other regression
  # models, as the poisson regression, gaussianity is not respected if n is not
  # large. For this reason, the parameters obtained are an approximation. 
  # The dispersion parameter phi, that we found in the general linear models section, is set to 1
  # Note: Fisher Scoring gives info on optimization attempts number. The program starts with some
  # values of beta, and then tries other values.
  
  plot(modC)  # Seems decent 
  # note the presence of another type of error. scale-location shows some values to 
  # go up and down, although we know this is the correct model
  # Note: Some regular-ish patterns may be seen when working with discrete data
  
  # Use the good model to make predictions 
  p1<-predict(modC)  
  # Calculates for each x the value of eta 
  # predict(modC) == coefficients(modC)[1] + coefficients(modC)[2] * x
  summary(p1)  
  # we compare the estimated values of eta with those obtained through 
  # log(y), which are the non-parametric eta hat which goes over the y points.
  # If the model fits well, in theory the two should be near
  plot(p1, log(y))
  abline(0,1)
  
  p2<-predict(modC,type="response")  # Estimates values of mu hat.
  # R will output the exp(linear correlation) values for each x,
  # instead of the values of eta. mu obviously represent
  # the best prediction of y
  
  summary(p2)
  plot(p2,y)  # Plot predicted vs simulated values of y 
  abline(0,1)  # Line that should be approximated by the 
  # datapoints. p2 predicts y from x, and since it gives values
  # comparable to y, y can be predicted by using x
  \end{minted}

\section{Example: simulating binomial data} %Binomial, Bernoulli?? #TODO
You can do the same with other distributions. This is the example for a
binomial distribution.

\begin{minted}{R}
  # Example 2, same xs and beta, and eta, change mu. Have to think to the link
  # function 
  x <- rnorm(100, 0, 2)  
  beta0 <- 1
  beta1 <- -2  
  eta <- beta0 + beta1 * x  
  mu <- exp(eta) / (1 + exp(eta))  # Use logit as the link function
  summary(mu)
  
  plot(eta, mu) #it is visible the "transformation" to a sigmoid line
  # of the linear function eta
  plot(x, mu)
  
  # The two plots are different, although they are really similar now.
  # For example, changing the value of beta1 to a negative value, we 
  # would obtain a reversed curve in the case of plot(x, mu), while instead 
  # the graph would remain unchanged in the case of plot(eta, mu)
  
  y <- rbinom(100, size=1, prob=mu) # Compute Bernoulli y values 
  plot(x, y) # only 0 and 1 values were generated
  table(y)
  
  modD <- glm(y~x, family="binomial")  # Create the model, default
  # is logit
  
  p4<-predict(modD) # Compute linear predictors 
  summary(p4)
  p5<-predict(modD,type="response") # Compute the fitted means, which 
  # are the probabilities 
  summary(p5)
\end{minted}
